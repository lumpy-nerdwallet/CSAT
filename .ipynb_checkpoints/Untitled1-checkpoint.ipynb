{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, math, collections, lda, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "STOPWORD_SET = set(nltk.corpus.stopwords.words(LANGUAGE)).add(\"even\") - set(['who', 'why', 'how', 'where', 'when', 'what', 'whom'])\n",
    "FILENAME = \"comments_raw_merged_with_page_data_190716.csv\"\n",
    "COLUMN_NAME = \"message\"\n",
    "\n",
    "def is_ASCII_string(string):\n",
    "    '''\n",
    "        Checks if string is ASCII-decodeable. Always run on pre-processed csv files!\n",
    "    '''\n",
    "    try:\n",
    "        string.decode('ascii')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ascii_substituted(string):\n",
    "    '''\n",
    "        Substitutes string to make it ASCII-readable. Currently very exception-driven, \n",
    "        but if there's a good package that does the dirty work please let @Lumpy know.\n",
    "    '''\n",
    "    return string.replace(\"\\\\n\", \" \").replace(\"&amp;\", \"&\").replace('&#039;', '\\'').replace(\"&quot;\", \"\\\"\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"\\xe2\\x80\\x93\", \"-\").replace(\"\\xe2\\x80\\x99\", \"\\'\").strip()\n",
    "\n",
    "\n",
    "### CLEAN YOUR DF!!\n",
    "def get_message_list(data_frame, column_name = \"message\"):\n",
    "    '''\n",
    "        Given a pre-CLEANED and pre-subsetted data frame, gets the message set from the data frame.\n",
    "    '''\n",
    "    return [message for message_l in data_frame[[column_name]].values.tolist() for message in message_l]            \n",
    "\n",
    "\n",
    "def is_eligible_word(token, stopwords = STOPWORD_SET, regex_string = \"^[^a-zA-Z0-9]+\"):\n",
    "    '''\n",
    "        Private helper function.\n",
    "        Check if word is eligible to be a token (i.e. not a forbidden regex, or in the stopword set).\n",
    "    '''\n",
    "    if re.match(regexPattern, word) or word in stopwords: \n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def append_NOTs(tokenized_message, stopwords = STOPWORD_SET):\n",
    "    '''\n",
    "        Private helper function to handle negations.\n",
    "        Given a List of words, returns a List of words with nots appended to the right words.\n",
    "    '''\n",
    "    new_message = []\n",
    "    for i in range(len(tokenized_message)):\n",
    "        if tokenized_message[i] in set([\"n\\'t\", \"n\\\"t\", \"no\", \"not\", \"didnt\"]) and i != (len(tokenized_message) - 1):\n",
    "            j = i + 1\n",
    "            while j < len(tokenized_message):\n",
    "                if tokenized_message[j] not in stopwords:\n",
    "                    tokenized_message[j] = \"not_\" + tokenized_message[j]\n",
    "                    break\n",
    "                j += 1\n",
    "        else:\n",
    "            new_message.append(tokenized_message[i])\n",
    "    return new_message\n",
    "\n",
    "\n",
    "def split_message_into_tokens(untokenized_message, ngram = 1, stopwords = STOPWORD_SET, regex_string = \"^[^a-zA-Z0-9]+\", handle_negations = True): \n",
    "    '''\n",
    "        Splits ONE message in the list of messages into words/tokens, in the process doing the following:\n",
    "        1) Changing to lowercase\n",
    "        2) Dealing with negations\n",
    "        3) N-gramming\n",
    "        4) Lemmatizing\n",
    "        5) Stopword-removal\n",
    "        Input: String.\n",
    "        Arguments: \n",
    "            ngram:              n in n-grams                        Default is 1\n",
    "            stopwords           Set of stopwords to remove          Default is the global variable\n",
    "            regex_string        String containing regex pattern     Default is \"^[^a-zA-Z0-9]+\"\n",
    "            handle_negations:   Do you want negations handled?      Default is True\n",
    "        Output: List of n-grams.\n",
    "    '''\n",
    "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    # Split into tokens\n",
    "    tokenized_message = nltk.word_tokenize(untokenized_message.lower())\n",
    "    # Deal with \"not\"s\n",
    "    if handle_negations:\n",
    "        tokenized_message = append_NOTs(tokenized_message, stopwords)\n",
    "    # Lemmatize, n-gram, stopword-removal.\n",
    "    if ngram == 1:\n",
    "        return [wordnet_lemmatizer.lemmatize(token) for token in tokenized_message if is_eligible_word(token, stopwords)]\n",
    "    else:\n",
    "        tokenized_message = ngrams(tokenized_message, ngram)\n",
    "        return [[wordnet_lemmatizer.lemmatize(token) for token in ngram_indiv if is_eligible_word(token, stopwords) not in stopwords] for ngram_indiv in tokenized_message if len(ngram_indiv) > 0]\n",
    "\n",
    "\n",
    "def tokenize(message_list, ngram = 1, stopwords = STOPWORD_SET, regex_string = \"^[^a-zA-Z0-9]+\", handle_negations = True, pos_tagset = \"no-pos\"):\n",
    "    '''\n",
    "        Tokenizes a list of strings. Assumes that message_list has been preprocessed.\n",
    "        Input: List of Strings.\n",
    "        Arguments:\n",
    "            ngram               n in n-grams                                                                                Default is 1\n",
    "            stopwords:          Set of stopwords to remove.                                                                 Default is the global variable.\n",
    "            regex_string        String containing regex pattern                                                             Default is \"^[^a-zA-Z0-9]+\"\n",
    "            handle_negations:   Do you want \"not X\" to be glued as \"not_X\"? Improves sentiment recognition.                 Default is True\n",
    "            pos_tagset:         For POS tagging. Current options are \"universal\", None (which maps to nltk.pos_tag) \n",
    "                                and \"no-pos\", which really means no POS tagging. Only applicable for unigrams.              Default is None\n",
    "        Output: List of List of Strings.\n",
    "        Note: will automatically change to lowercase, and will automatically lemmatize.\n",
    "    '''\n",
    "    if ngram == 1 and (pos_tagset == None or pos_tagset == \"universal\"): ## Will perform POS-tagging\n",
    "        return [nltk.pos_tag(split_message_into_tokens(untokenized_message, ngram = ngram, stopwords = stopwords, regex_string = \"^[^a-zA-Z0-9]+\", handle_negations = True), pos_tagset = pos_tagset) for untokenized_message in message_list]\n",
    "    else:\n",
    "        return [split_message_into_tokens(untokenized_message, ngram = ngram, stopwords = stopwords, regex_string = \"^[^a-zA-Z0-9]+\", handle_negations = True) for untokenized_message in message_list]\n",
    "\n",
    "## STEP 1: PREPROCESSING\n",
    "df = pd.read_csv(FILENAME, delimiter = ',')\n",
    "faulty_arrays = []\n",
    "good_arrays = []\n",
    "\n",
    "for i in range(len(df[[COLUMN_NAME]].values.tolist())):\n",
    "    message = df[COLUMN_NAME].values.tolist()[i]\n",
    "    if not is_ASCII_string(ascii_substituted(str(message))):\n",
    "        faulty_arrays.append(i)\n",
    "    else:\n",
    "        df.set_value(i, COLUMN_NAME, ascii_substituted(str(message)))\n",
    "        good_arrays.append(i)\n",
    "        \n",
    "df = df.iloc[good_arrays, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORD_SET = set(nltk.corpus.stopwords.words(LANGUAGE)).add(\"even\") - set(['who', 'why', 'how', 'where', 'when', 'what', 'whom'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1151\n",
      "1181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##messageToken = obtain_Message_Token_List(df, \"Credit Cards\", posTag = False, regexPattern = \"^[^?a-zA-Z]+\", specialNotToRemove = newWords)\n",
    "\n",
    "category = \"Credit Cards\"\n",
    "df = df.loc[df['page_vertical_tx'] == category]\n",
    "df[[\"message\"]].values.tolist()\n",
    "#[replaceString(str(message)) for messageL in df[[messageColumnName]].values.tolist() for message in messageL if isASCIIString(replaceString(str(message)))]            \n",
    "for i in range(len(df[[\"message\"]].values.tolist())):\n",
    "    message = df[[\"message\"]].values.tolist()[i][0]\n",
    "    if not isASCIIString(replaceString(str(message))):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_Message_Token_List(df, category, ngram = 1, split_by = 'page_vertical_tx', messageColumnName = \"message\", lang = \"english\", regexPattern = \"^[^a-zA-Z0-9]+\", pos_tagset = 'universal', specialNotToRemove = set([]), isNormalized = True, isLemmatized = True, stopWordsRemoved = True, handleNegations = True, puncRemoved = True, posTag = True):\n",
    "    ## Step 1: get the messages perfectly, dealing with weird cases\n",
    "    messageSet = get_message_set(df, category, split_by = 'page_vertical_tx', messageColumnName = \"message\")\n",
    "    if len(messageSet) == 0:\n",
    "        return []\n",
    "    else:    \n",
    "        ## Step 2: Tokenize and (optionally) normalize\n",
    "        messageToken = tokenize_message_set(messageSet, isNormalized)\n",
    "        ## Step 3: Remove stop words, punctuation\n",
    "        ## Step 4: Lemmatize and N-gram (kind of with step 3 too)\n",
    "        ## Step 5: POS-tag?\n",
    "        if ngram == 1 and posTag:\n",
    "            return [nltk.pos_tag(get_ngram_set(message, ngram, isLemmatized = isLemmatized, stopWordsRemoved = stopWordsRemoved, puncRemoved = puncRemoved, regexPattern = regexPattern, lang = lang, specialNotToRemove = specialNotToRemove, handleNegations = handleNegations), tagset = pos_tagset) for message in messageToken]\n",
    "        else:\n",
    "            return [get_ngram_set(message, ngram, isLemmatized = isLemmatized, stopWordsRemoved = stopWordsRemoved, puncRemoved = puncRemoved, regexPattern = regexPattern, lang = lang, specialNotToRemove = specialNotToRemove, handleNegations = handleNegations) for message in messageToken]\n",
    "\n",
    "            \n",
    "def get_message_set(df, category, split_by = 'page_vertical_tx', messageColumnName = \"message\"):\n",
    "    if category != None:\n",
    "        df = df.loc[df[split_by] == category]\n",
    "    if len(df.shape) > 0 and df.shape[0] == 0:\n",
    "        print(\"Category \\'\" + str(category) + \"\\' has no rows. Check spelling.\")\n",
    "        return []\n",
    "    else:\n",
    "        return [replaceString(str(message)) for messageL in df[[messageColumnName]].values.tolist() for message in messageL if isASCIIString(replaceString(str(message)))]            \n",
    "\n",
    "def isASCIIString(string):\n",
    "    try:\n",
    "        string.decode('ascii')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Just some ASCII coding issues\n",
    "def replaceString(string):\n",
    "    return string.replace(\"\\\\n\", \" \").replace(\"&amp;\", \"&\").replace('&#039;', '\\'').replace(\"&quot;\", \"\\\"\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"\\xe2\\x80\\x93\", \"-\").replace(\"\\xe2\\x80\\x99\", \"\\'\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Where do I apply for your Costco Anywhere Visa\\xc2\\xae Card by Citi's? Can I apply at the Costco Customer Service counter?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = df.iloc[[1181]][[\"message\"]].values.tolist()[0][0]\n",
    "replaceString(str(msg)).replace(\"\\xe2\\x80\\x93\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
