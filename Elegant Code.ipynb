{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, math, collections, lda, random, json, wordcloud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.util import ngrams\n",
    "\n",
    "FILENAME = \"comments_raw_merged_with_page_data_190716.csv\"\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "STOPWORD_SET = (set(nltk.corpus.stopwords.words(LANGUAGE)) | set(['even', '.', ','])) - set(['who', 'why', 'how', 'where', 'when', 'what', 'whom']) \n",
    "COLUMN_NAME = \"message\"\n",
    "LDA_TOPIC_NUMBER = 3 # Kind of trialed and errored.\n",
    "LDA_ITER = 500 # Don't change this!\n",
    "LDA_RANDOM_STATE = 1 # Don't change this!\n",
    "SEED_NUMBER = 10\n",
    "THANK_YOU_WORD = 'thank'\n",
    "THANKFUL_CATEGORY = 1\n",
    "POST_ID_COLUMN_NAME = 'post_id'\n",
    "TF_WORD_THRESHOLD = 5\n",
    "MAX_ARTICLE_LIMIT = 3\n",
    "ARTICLE_ALERT_SCORE_CUTOFF = 0.7\n",
    "MAX_COMMENT_LIMIT = 3\n",
    "COMMENT_SCORE_CUTOFF = 0.1\n",
    "URL_PATH_COLUMN_NAME = \"page_path_tx\"\n",
    "MIN_CLUSTER_COUNT = -1\n",
    "KEYWORDS_PER_CLUSTER = 5\n",
    "WORDCLOUD_MAX_WORDS = 100\n",
    "WORDCLOUD_HEIGHT = 400\n",
    "WORDCLOUD_WIDTH = 800\n",
    "COMMENTS_PER_POSTS_RATIO = 100 \n",
    "MIN_COMMENTS_PER_POST = 10\n",
    "AVERAGE_CSAT_SCORE = 0.5\n",
    "MAX_KEYWORD_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_ASCII_string(string):\n",
    "\t'''\n",
    "\t\tChecks if string is ASCII-decodeable. Always run on pre-processed csv files!\n",
    "\t'''\n",
    "\ttry:\n",
    "\t\tstring.decode('ascii')\n",
    "\t\treturn True\n",
    "\texcept:\n",
    "\t\treturn False\n",
    "\n",
    "\n",
    "def ascii_substituted(string):\n",
    "\t'''\n",
    "\t\tSubstitutes string to make it ASCII-readable. Currently very exception-driven, \n",
    "\t\tbut if there's a good package that does the dirty work please let @Lumpy know.\n",
    "\t'''\n",
    "\treturn string.replace(\"\\\\n\", \" \").replace(\"&amp;\", \"&\").replace('&#039;', '\\'').replace(\"&quot;\", \"\\\"\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"\\xe2\\x80\\x93\", \"-\").replace(\"\\xe2\\x80\\x99\", \"\\'\").strip()\n",
    "\n",
    "### CLEAN YOUR DF!!\n",
    "def get_message_list(data_frame, column_name = \"message\"):\n",
    "\t'''\n",
    "\t\tGiven a pre-CLEANED and pre-subsetted data frame, gets the message set from the data frame.\n",
    "\t'''\n",
    "\treturn [message for message_l in data_frame[[column_name]].values.tolist() for message in message_l]            \n",
    "\n",
    "\n",
    "def is_eligible_word(token, stopwords = STOPWORD_SET, regex_string = \"^[^a-zA-Z0-9]+\"):\n",
    "\t'''\n",
    "\t\tPrivate helper function.\n",
    "\t\tCheck if word is eligible to be a token (i.e. not a forbidden regex, or in the stopword set).\n",
    "\t'''\n",
    "\tpattern = re.compile(regex_string)\n",
    "\tif pattern.match(token) or token in stopwords: \n",
    "\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "\n",
    "def append_NOTs(tokenized_message, stopwords = STOPWORD_SET):\n",
    "\t'''\n",
    "\t\tPrivate helper function to handle negations.\n",
    "\t\tGiven a List of words, returns a List of words with nots appended to the right words.\n",
    "\t'''\n",
    "\tnew_message = []\n",
    "\tfor i in range(len(tokenized_message)):\n",
    "\t\tif tokenized_message[i] in set([\"n\\'t\", \"n\\\"t\", \"no\", \"not\", \"didnt\"]) and i != (len(tokenized_message) - 1):\n",
    "\t\t\tj = i + 1\n",
    "\t\t\twhile j < len(tokenized_message):\n",
    "\t\t\t\tif tokenized_message[j] not in stopwords:\n",
    "\t\t\t\t\ttokenized_message[j] = \"not_\" + tokenized_message[j]\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tj += 1\n",
    "\t\telse:\n",
    "\t\t\tnew_message.append(tokenized_message[i])\n",
    "\treturn new_message\n",
    "\n",
    "\n",
    "def split_message_into_tokens(untokenized_message, ngram = 1, stopwords = STOPWORD_SET, regex_string = \"^[^a-zA-Z0-9]+\", handle_negations = True): \n",
    "\t'''\n",
    "\t\tSplits ONE message in the list of messages into words/tokens, in the process doing the following:\n",
    "\t\t1) Changing to lowercase\n",
    "\t\t2) Dealing with negations\n",
    "\t\t3) N-gramming\n",
    "\t\t4) Lemmatizing\n",
    "\t\t5) Stopword-removal\n",
    "\t\tInput: String.\n",
    "\t\tArguments: \n",
    "\t\t\tngram:              n in n-grams                        Default is 1\n",
    "\t\t\tstopwords           Set of stopwords to remove          Default is the global variable\n",
    "\t\t\tregex_string        String containing regex pattern     Default is \"^[^a-zA-Z0-9]+\"\n",
    "\t\t\thandle_negations:   Do you want negations handled?      Default is True\n",
    "\t\tOutput: List of n-grams.\n",
    "\n",
    "\t\tNote: A PorterStemmer doesn't work very well, because the lemmatizer often is unable to re-stem many words. A better method would be just to put a lemmatizer, with default POS \"v\" for verb.\n",
    "\t'''\n",
    "\twordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\t#stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\t# Split into tokens\n",
    "\ttokenized_message = nltk.word_tokenize((ascii_substituted(str(untokenized_message))).lower())\n",
    "\t# Deal with \"not\"s\n",
    "\tif handle_negations:\n",
    "\t\ttokenized_message = append_NOTs(tokenized_message, stopwords)\n",
    "\t# Lemmatize, n-gram, stopword-removal.\n",
    "\tif ngram == 1:\n",
    "\t\treturn [wordnet_lemmatizer.lemmatize(token, \"v\") for token in tokenized_message if is_eligible_word(token, stopwords, regex_string)]\n",
    "\telse:\n",
    "\t\ttokenized_message = ngrams(tokenized_message, ngram)\n",
    "\t\tfiltered_tokenized_message = [\" \".join(str(wordnet_lemmatizer.lemmatize(token, \"v\")) for token in ngram_indiv if is_eligible_word(token, stopwords, regex_string)) for ngram_indiv in tokenized_message]\n",
    "\t\treturn [final_token for final_token in filtered_tokenized_message if len(final_token) > 0] \n",
    "\n",
    "\n",
    "def tokenize(message_list, ngram = 1, stopwords = STOPWORD_SET, regex_string = \"^[^?a-zA-Z0-9]+\", handle_negations = True, pos_tagset = \"no-pos\"):\n",
    "\t'''\n",
    "\t\tTokenizes a list of strings. Assumes that message_list has been preprocessed.\n",
    "\t\tInput: List of Strings.\n",
    "\t\tArguments:\n",
    "\t\t\tngram               n in n-grams                                                                                Default is 1\n",
    "\t\t\tstopwords:          Set of stopwords to remove.                                                                 Default is the global variable.\n",
    "\t\t\tregex_string        String containing regex pattern                                                             Default is \"^[^a-zA-Z0-9]+\"\n",
    "\t\t\thandle_negations:   Do you want \"not X\" to be glued as \"not_X\"? Improves sentiment recognition.                 Default is True\n",
    "\t\t\tpos_tagset:         For POS tagging. Current options are \"universal\", None (which maps to nltk.pos_tag) \n",
    "\t\t\t\t\t\t\t\tand \"no-pos\", which really means no POS tagging. Only applicable for unigrams.              Default is None\n",
    "\t\tOutput: List of List of Strings.\n",
    "\t\tNote: will automatically change to lowercase, and will automatically lemmatize.\n",
    "\t'''\n",
    "\tif ngram == 1 and (pos_tagset == None or pos_tagset == \"universal\"): ## Will perform POS-tagging\n",
    "\t\treturn [nltk.pos_tag(split_message_into_tokens(untokenized_message, ngram = ngram, stopwords = stopwords, regex_string = regex_string, handle_negations = handle_negations), pos_tagset = pos_tagset) for untokenized_message in message_list]\n",
    "\telse:\n",
    "\t\treturn [split_message_into_tokens(untokenized_message, ngram = ngram, stopwords = stopwords, regex_string = regex_string, handle_negations = handle_negations) for untokenized_message in message_list]\n",
    "\n",
    "\n",
    "def get_unnested_list(word_list):\n",
    "\t'''\n",
    "\t\tGets an un-nested list from a nested list of words. Deals in principle with ngrams with n > 1.\n",
    "\t'''\n",
    "\tif len(word_list) == 0 or (type(word_list[0]) not in set([list, dict, set, tuple])):\n",
    "\t\treturn word_list\n",
    "\telse:\n",
    "\t\treturn get_unnested_list([word for word_array in word_list for word in word_array])\n",
    "\n",
    "\t\n",
    "def get_token_to_index_dict(token_list, threshold = TF_WORD_THRESHOLD):\n",
    "\t'''\n",
    "\t\tGets a dict mapping tokens to indices, given an (unnested) list of tokens.\n",
    "\t'''\n",
    "\ttoken_list = sorted([item[0] for item in collections.Counter(token_list).items() if item[1] >= threshold])\n",
    "\treturn {token_list[i]:i for i in range(len(token_list))}\n",
    "\n",
    "\t\n",
    "def get_ordered_array_of_tokens(token_to_index_dict):\n",
    "\t''' \n",
    "\t\tGets back the ordered array of tokens, with the index for each token being the value in the dict.\n",
    "\t\tObviously assumes that the dict's values are all unique and range from 0 to len(dict) - 1. \n",
    "\t'''\n",
    "\treturn [token_index_tuple[0] for token_index_tuple in sorted(token_to_index_dict.items(), key = lambda x: x[1])]\n",
    "\n",
    "\n",
    "def get_TF_matrix(tokenized_message_list, token_to_index_dict, count_only_once = set([])):\n",
    "\t## FIXED: NOW matrix is n, p dimensions where n is number of training documents and p rightfully is predictor (word)\n",
    "\t''' \n",
    "\t\tReturns a NumPy Matrix where the (i, j)th position represents the number of times word j is present in document i. \n",
    "\t\tInput: \n",
    "\t\t\ttokenized_message_list:     List of List of tokens.\n",
    "\t\t\ttoken_to_index_dict:        Dict mapping token to index\n",
    "\t\tArguments:\n",
    "\t\t\tcount_only_once:            Set of tokens to only count once for each document.         Default: empty set\n",
    "\t\tOutput: Term-Frequency Matrix    \n",
    "\t'''\n",
    "\tn_documents = len(tokenized_message_list)\n",
    "\tn_words = len(token_to_index_dict)\n",
    "\ttf_matrix = np.zeros((n_documents, n_words), dtype=np.int)\n",
    "\tfor i in range(n_documents):\n",
    "\t\ttokenized_message = tokenized_message_list[i]\n",
    "\t\tfor token in tokenized_message:\n",
    "\t\t\tif token in token_to_index_dict:\n",
    "\t\t\t\tj = token_to_index_dict[token]\n",
    "\t\t\t\tif token in count_only_once:\n",
    "\t\t\t\t\ttf_matrix[i, j] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttf_matrix[i, j] += 1\n",
    "\treturn tf_matrix\n",
    "\n",
    "\t\n",
    "def get_IDF_score(tf_matrix):\n",
    "\t'''\n",
    "\t\tGiven the Term-Frequency Matrix, counts the inverse-document-frequency score for the term.\n",
    "\t\tCalculated as IDF (token) = ln( N / (number of docs token is in + 1)),\n",
    "\t\twhere N is number of document, and the +1 is for smoothing purposes.\n",
    "\t\tReturns a List of floats.\n",
    "\t'''\n",
    "\tidf_score = []\n",
    "\tn_documents = tf_matrix.shape[0]\n",
    "\tfor j in range(tf_matrix.shape[1]):\n",
    "\t\tn_appearing_docs = len(np.where(tf_matrix[:,j] > 0)[0])\n",
    "\t\tidf = math.log(float(n_documents) / (n_appearing_docs) + 1)\n",
    "\t\tidf_score.append(idf)\n",
    "\treturn idf_score\n",
    "\n",
    "\n",
    "def get_TFIDF_matrix(tf_matrix, idf_score, multiple_counts = True, normalized = True):\n",
    "\t## Assume L2 normalization. I'm kinda lazy to implement L1\n",
    "\t## MultipleCounts = False means all nonzero iterms in tf_matrix becomes 1\n",
    "\t## Add functionality to only singularcount a subset of words\n",
    "\t'''\n",
    "\t\tGiven the TF matrix and the IDF score, calculates the TF-IDF matrix.\n",
    "\t\tInputs: \n",
    "\t\t\ttf_matrix: A (n-documents * p-words) matrix.\n",
    "\t'''\n",
    "\tn_rows, n_cols = tf_matrix.shape\n",
    "\ttf_idf = np.zeros(tf_matrix.shape)\n",
    "\tfor i in range(n_rows):\n",
    "\t\tfor j in range(n_cols):\n",
    "\t\t\ttf_idf[i, j] = float(tf_matrix[i, j] if multiple_counts else min(tf_matrix[i, j], 1)) * idf_score[j]\n",
    "\tif normalized:\n",
    "\t\ttf_idf = normalize(tf_idf, axis = 1, norm = 'l2')\n",
    "\treturn tf_idf\n",
    "\n",
    "\n",
    "#### If you want to rework the algorithm, look here\n",
    "def get_LDA_scores(tf_matrix, token_to_index_dict, seed_no = SEED_NUMBER, n_topics = LDA_TOPIC_NUMBER, n_iter = LDA_ITER, lda_random_state = LDA_RANDOM_STATE):\n",
    "\t'''\n",
    "\t\tComputes individual comment scores via LDA, as well as returns the index for which sentiment was positive\n",
    "\t'''\n",
    "\trandom.seed(seed_no)\n",
    "\tmodel = lda.LDA(n_topics = LDA_TOPIC_NUMBER, n_iter = LDA_ITER, random_state = LDA_RANDOM_STATE)\n",
    "\tmodel.fit(tf_matrix)\n",
    "\t## Detects the one with the \"thank you\"s. If \"thank\" doesn't exist, it will go berserk - but that's the point!\n",
    "\ttopic_word = model.topic_word_\n",
    "\tif THANK_YOU_WORD in token_to_index_dict:\n",
    "\t\tindex_of_positive_sentiment_topic = np.argsort(model.topic_word_[:, token_to_index_dict[THANK_YOU_WORD]])[:1:-1][0]\n",
    "\telse:\n",
    "\t\tindex_of_positive_sentiment_topic = THANKFUL_CATEGORY ### SOUND OFF\n",
    "\t\tprint(\"\\'thank\\'' was not found. Use with caution.\")\n",
    "\treturn np.dot(tf_matrix, np.transpose(model.components_)), index_of_positive_sentiment_topic # this gives you the full topic scores for each item in the DF\n",
    "#### Edit a bit of the next function too.\n",
    "\n",
    "\n",
    "def compute_final_scores(raw_comment_scores, LDA_positive_category):\n",
    "\t'''\n",
    "\t\tGiven the TF matrix (optionally tf_idf) and the token_to_index_dict:\n",
    "\t\tGets you back comment scores for all comments. Use them to rank how high articles should go.\n",
    "\t\tPASS 1: Vanilla LDA, implemented 28 July 2016. Calls the get_LDA_scores function\n",
    "\t'''\n",
    "\tif LDA_positive_category < 0 or LDA_positive_category >= raw_comment_scores.shape[1]:\n",
    "\t\treturn np.sum(raw_comment_scores, axis = 1)\n",
    "\treturn np.sum(np.delete(raw_comment_scores, LDA_positive_category, axis = 1), axis = 1)\n",
    "\t \n",
    "\n",
    "def get_posts_scores(df, raw_comment_scores, LDA_positive_category, column_to_group_by = POST_ID_COLUMN_NAME):\n",
    "\t'''\n",
    "\t\tGiven the data frame, raw comment scores and the positive LDA category, gets back a numpy matrix with post_id and post scores, sorted by the biggest to smallest post score (bigger = worse).\n",
    "\t'''\n",
    "\tfinal_comment_scores = compute_final_scores(raw_comment_scores, LDA_positive_category)\n",
    "\tposts = df.groupby(column_to_group_by)\n",
    "\tfinal_post_scores = np.zeros((len(posts.indices), 2))\n",
    "\tindex = 0\n",
    "\tfor post_id, comment_indices in posts.indices.iteritems():\n",
    "\t\tfinal_post_scores[index, 0] = post_id\n",
    "\t\tfinal_post_scores[index, 1] = np.sum(final_comment_scores[comment_indices])\n",
    "\t\tindex += 1\n",
    "\treturn final_post_scores[final_post_scores[:, 1].argsort()[::-1]]\n",
    "\n",
    "#### get_posts_scores_v2 is an attempt to work on the current get_posts_scores algorithm. Read more in the docstring\n",
    "def get_posts_scores_v2(df, raw_comment_scores, LDA_positive_category, column_to_group_by = POST_ID_COLUMN_NAME):\n",
    "\t'''\n",
    "\t\tThis attempt will calculate the posts score based on the formula above:\n",
    "\t\tPost score = [sum (bad_quality_score) / (sum (bad_quality_score) + sum (good_quality_score))]. \n",
    "\t\tSome tweaks will have to be done to factor in the number of comments.\n",
    "\t\tThere will be a couple of comments in the raw_comment_scores with 0 scores across the board (consider them as \"junk comments\"). Just make sure you know that.       \n",
    "\t'''\n",
    "\tposts = df.groupby(column_to_group_by)\n",
    "\tfinal_post_scores = np.zeros((len(posts.indices), 2))\n",
    "\tindex = 0\n",
    "\tfor post_id, comment_indices in posts.indices.iteritems():\n",
    "\t\tfinal_post_scores[index, 0] = post_id\n",
    "\t\tpost_good_and_bad_score = np.sum(raw_comment_scores[comment_indices, :])\n",
    "\t\tcsat_score = calculate_csat_scores(df.iloc[comment_indices])\n",
    "\t\tif post_good_and_bad_score > 0 and len(comment_indices) > max(MIN_COMMENTS_PER_POST, (raw_comment_scores.shape[0] / COMMENTS_PER_POSTS_RATIO)) and csat_score < AVERAGE_CSAT_SCORE:\n",
    "\t\t\tfinal_post_scores[index, 1] = np.true_divide(np.sum(np.delete(raw_comment_scores[comment_indices, :], LDA_positive_category, axis = 1)), np.sum(raw_comment_scores[comment_indices, :]))\n",
    "\t\telse:\n",
    "\t\t\tfinal_post_scores[index, 1] = 0\n",
    "\t\tindex += 1\n",
    "\treturn final_post_scores[final_post_scores[:, 1].argsort()[::-1]]\n",
    "\n",
    "\n",
    "''' Kind of depreciated for now, but if ever it comes useful...\n",
    "def get_posts_scores_csat(df):\n",
    "\tposts = df.groupby(POST_ID_COLUMN_NAME)\n",
    "\tcsat_scores = np.zeros((len(posts.indices), 2))\n",
    "\tindex = 0\n",
    "\tfor post_id, comment_indices in posts.indices.iteritems():\n",
    "\t\tcsat_scores[index, 0] = post_id\n",
    "\t\tcsat_scores[index, 1] = calculate_csat_scores(df.iloc[comment_indices]) if len(comment_indices) > 10 else None\n",
    "\t\tindex += 1\n",
    "\tfinal_post_scores = csat_scores[csat_scores[:, 1].argsort()]\n",
    "\treturn final_post_scores[~np.isnan(final_post_scores[:, 1])]\n",
    "'''\n",
    "\n",
    "def calculate_csat_scores(df):\n",
    "\t'''\n",
    "\t\tA function to get back the CSat scores for a particular post_id, given the subsetted data frame.\n",
    "\t'''\n",
    "\tcounts = df[\"helpful_yn\"].value_counts()\n",
    "\tdf_nrow = df.shape[0]\n",
    "\tif df_nrow > 0 and \"yes\" in counts.index:\n",
    "\t\treturn float(counts[np.where(counts.index == \"yes\")[0][0]]) / df_nrow\n",
    "\telse:\n",
    "\t\treturn 0 # kind of a catch-all. may change it to -1 at some point\n",
    "\n",
    "\n",
    "def find_index_of_positive_sentiment_topic(LDA_scores, token_to_index_dict, thank_you_word = THANK_YOU_WORD):\n",
    "\t'''\n",
    "\t\tIn anticipation of future expansion:\n",
    "\t\tGiven a token_to_index dict, and the saved LDA scores, manually find the index of the topic that embodies positive sentiment.\n",
    "\t\tWill be extremely useful in grading scores.\n",
    "\t'''\n",
    "\tassert LDA_scores.shape[0] == len(token_to_index_dict)\n",
    "\treturn np.argsort(LDA_scores[:, token_to_index_dict[thank_you_word]])[:1:-1][0]\n",
    "\n",
    "\n",
    "\n",
    "## TO FIX\n",
    "def obtain_frequent_keywords(indices_of_comments, raw_comment_scores, tf_matrix, idf_score, token_to_index_dict, min_cluster_count = MIN_CLUSTER_COUNT, seed_number = SEED_NUMBER, KEYWORDS_PER_CLUSTER = KEYWORDS_PER_CLUSTER):\n",
    "\t'''\n",
    "\t\tFirst cluster into k different clusters based on LDA score.\n",
    "\t\tNext, pick out the top few clusters (assumption, same words/similar words will have the same kind of scores.)\n",
    "\t\tWithin the top few clusters, pick out the top-most TF-IDF scores, assuming you put it all together. \n",
    "\t\t\n",
    "\t\tInputs:\n",
    "\t\t\tindices_of_comments: the DF indices of comments, so as to access the tf_matrix.\n",
    "\t\t\traw_comment_scores: the np.array of LDA comment scores (in full), so then indices_of_comments can subset the relevant ones\n",
    "\t\t\ttf_matrix: the Term Frequency matrix. (n * p, where n should be the same DF indices, and p the number of words)\n",
    "\t\t\tidf_score: idf score for the DF in full\n",
    "\t\t\ttoken_to_index_dict: the dictionary mapping out token to index\n",
    "\t'''\n",
    "\trelevant_comments_normalized_scores = np.true_divide(raw_comment_scores[indices_of_comments], np.sum(raw_comment_scores[indices_of_comments], axis = 1)[:, None])\n",
    "\t# clearing out the 0s\n",
    "\tindices_of_comments = indices_of_comments[~np.all(np.isnan(relevant_comments_normalized_scores), axis = 1)]\n",
    "\trelevant_comments_normalized_scores = relevant_comments_normalized_scores[~np.all(np.isnan(relevant_comments_normalized_scores), axis = 1)]\n",
    "\tk = int(math.floor(math.sqrt(len(indices_of_comments)))) # k = floor(sqrt(# of comments))\n",
    "\tif min_cluster_count < 0:\n",
    "\t\tmin_cluster_count = k\n",
    "\tif seed_number < 0: # there's a way to disable seeding. Why would you not want seeds though? \n",
    "\t\trandom.seed(seed_number)\n",
    "\tkm = KMeans(n_clusters = k, n_init = LDA_ITER)\n",
    "\tkm.fit(relevant_comments_normalized_scores)\n",
    "\tcluster_counts = collections.Counter(km.labels_).most_common(k)\n",
    "\tfrequent_clusters = [count[0] for count in cluster_counts if (count[1] > min_cluster_count or count[1] >= cluster_counts[0][1])]\n",
    "\tfrequent_clusters = frequent_clusters[0:min(len(frequent_clusters), MAX_KEYWORD_CLUSTERS)]\n",
    "\tarray_of_tokens = get_ordered_array_of_tokens(token_to_index_dict)\n",
    "\tfrequent_keywords = {}\n",
    "\ttopic_number = 1   \n",
    "\tfor cluster in frequent_clusters:\n",
    "\t\tcluster_indices = indices_of_comments[np.where(np.array(km.labels_) == cluster)]\n",
    "\t\ttf_sum = np.sum(tf_matrix[cluster_indices, :], axis = 0)\n",
    "\t\ttf_idf_scores = tf_sum * np.array(idf_score)\n",
    "\t\tcluster_name = \"Keywords of Topic \" + str(topic_number)\n",
    "\t\ttopic_number += 1\n",
    "\t\tfrequent_keywords[cluster_name] = [array_of_tokens[i] for i in (tf_idf_scores.argsort()[::-1][0:KEYWORDS_PER_CLUSTER]).tolist()]\n",
    "\treturn frequent_keywords\n",
    "\n",
    "\n",
    "\n",
    "def get_articles(df, final_post_scores, raw_comment_scores, vertical_name, tf_matrix, idf_score, token_to_index_dict, max_article_limit = MAX_ARTICLE_LIMIT, max_comment_limit = MAX_COMMENT_LIMIT, ARTICLE_ALERT_SCORE_CUTOFF = ARTICLE_ALERT_SCORE_CUTOFF, comment_score_cutoff = COMMENT_SCORE_CUTOFF):\n",
    "\tposts = df.groupby(POST_ID_COLUMN_NAME)\n",
    "\timportant_comment_scores = compute_final_scores(raw_comment_scores, -1) # length - full df\n",
    "\tfinal_category_output = []\n",
    "\tfor i in range(min(max_article_limit, final_post_scores.shape[0])):\n",
    "\t\tif final_post_scores[i, 1] <= ARTICLE_ALERT_SCORE_CUTOFF:\n",
    "\t\t\tbreak\n",
    "\t\tpost_id = int(final_post_scores[i, 0])\n",
    "\t\tpost_alert_score = final_post_scores[i, 1]\n",
    "\t\tindices_of_comments = posts.indices[post_id] \n",
    "\t\tpost_url = df[URL_PATH_COLUMN_NAME].iloc[indices_of_comments[0]] # any comment will do. They share the same URL anyway.\n",
    "\t\tpost_csat_score = calculate_csat_scores(df.iloc[indices_of_comments])\n",
    "\t\tcomment_count = len(indices_of_comments) \n",
    "\t\tpost_info = {\"post_id\": post_id, \"post_url\": post_url, \"post_alert_score\": post_alert_score, \"post_csat_score\": post_csat_score, \"comment_count\": comment_count}\n",
    "\t\tcomment_array = []\n",
    "\t\tapplicable_comment_scores = np.transpose(np.vstack((np.array(indices_of_comments), np.array(important_comment_scores[indices_of_comments])))) # np array with df index, and its comment score\n",
    "\t\tapplicable_comment_scores = applicable_comment_scores[applicable_comment_scores[:, 1].argsort()[::-1]] # these should be index of df\n",
    "\t\tfor j in range(min(max_comment_limit, applicable_comment_scores.shape[0])): \n",
    "\t\t\tif applicable_comment_scores[j, 1] <= comment_score_cutoff:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tcomment_index = int(applicable_comment_scores[j, 0])\n",
    "\t\t\tlog_id = df[\"log_id\"].iloc[comment_index]\n",
    "\t\t\tcomment_text = ascii_substituted(str(df[COLUMN_NAME].iloc[comment_index]))\n",
    "\t\t\tcomment_array.append({\"log_id\": log_id, \"comment_text\": comment_text})\n",
    "\t\tpost_info[\"comment\"] = comment_array\n",
    "\t\tpost_info[\"keywords\"] = obtain_frequent_keywords(indices_of_comments, raw_comment_scores, tf_matrix, idf_score, token_to_index_dict)\n",
    "\t\tfinal_category_output.append(post_info)\n",
    "\treturn {\"vertical\": vertical_name, \"summary\": final_category_output}\n",
    "\n",
    "\n",
    "'''\n",
    "An example of what the JSON format would be like:\n",
    "{\t\"vertical\": \"Credit Cards\",\n",
    "\t\"summary\": [\n",
    "\t\t{\t\"post_id\": \"538\", \n",
    "\t\t\t\"post_csat_score\": 0.1, \n",
    "\t\t\t\"comment_count\": 100,\n",
    "\t\t\t\"post_alert_score\": 0.802,\n",
    "\t\t\t\"post_url\": \"/blog/make-donald-drumpf-again\", \n",
    "\t\t\t\"comment\": [\n",
    "\t\t\t\t{\t\"log_id\": 24601,\n",
    "\t\t\t\t\t\"comment_text\": \"father father father help us\"\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\t]}, \n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "##### WORDCLOUD\n",
    "def generate_wordcloud(df, vertical_name, ngram = 2, regex_string = \"^[^a-zA-Z0-9]+\", height = WORDCLOUD_HEIGHT, width = WORDCLOUD_WIDTH):\n",
    "\t'''\n",
    "\t\tGenerates the wordcloud. \n",
    "\t\tInput: Pandas dataframe\n",
    "\t\tOutput: A wordcloud, saved in whatever relevant drive\n",
    "\t'''\n",
    "\t# Step 1: Generate token count.\n",
    "\t\t# (ngram = 2 works. More than that doesn't add more)\n",
    "\ttokenized_positive = get_unnested_list(tokenize(get_message_list(df[df['helpful_yn'] == \"yes\"]), ngram = ngram, regex_string = regex_string))\n",
    "\ttoken_counts_positive = pd.DataFrame(collections.Counter(tokenized_positive).items())\n",
    "\ttoken_counts_positive[\"Percentage\"] = token_counts_positive[1] / token_counts_positive[1].sum()\n",
    "\ttoken_counts_positive.rename(columns = {0:\"Bigram\", 1:\"Counts\"}, inplace = True)\n",
    "\ttokenized_negative = get_unnested_list(tokenize(get_message_list(df[df['helpful_yn'] == \"no\"]), ngram = ngram, regex_string = regex_string))\n",
    "\ttoken_counts_negative = pd.DataFrame(collections.Counter(tokenized_negative).items())\n",
    "\ttoken_counts_negative[\"Percentage\"] = token_counts_negative[1] / token_counts_negative[1].sum()\n",
    "\ttoken_counts_negative.rename(columns = {0:\"Bigram\", 1:\"Counts\"}, inplace = True)\n",
    "\n",
    "\t# Step 2: Calculate percent difference between words in a \"helpful CSat\" vs an \"unhelpful CSat\". \n",
    "\ttoken_counts = token_counts_positive.merge(token_counts_negative, how = \"outer\", on = \"Bigram\").fillna(0)\n",
    "\ttoken_counts[\"Difference\"] = token_counts[\"Percentage_x\"] - token_counts[\"Percentage_y\"]\n",
    "\t## DO WE WANT TO ADD POS tag? An option can be done with the following code:\n",
    "\t# token_counts[\"POS\"] = token_counts[\"Bigram\"].apply(lambda x: nltk.pos_tag([x], tagset = \"universal\")[0][1]) \n",
    "\t# No simple way to deal with POS tagging, because POS tagger is not very good especially for common words. \n",
    "\t# Potential fix: weigh scores with IDF scores.\n",
    "\t\t# Complication: it seems like the WordCloud package already deals with it in some random form...to investigate\n",
    "\n",
    "\t# Step 3: Generate wordcloud based on that, and save to file\n",
    "\twc = wordcloud.WordCloud(height = height, width = width)\n",
    "\twc.generate_from_frequencies(token_counts[[\"Bigram\", \"Difference\"]].sort_values(by = \"Difference\", ascending = False).iloc[0:WORDCLOUD_MAX_WORDS].values.tolist())\n",
    "\twc.to_file(vertical_name + \"_positive_wordcloud.png\")\n",
    "\twc = wordcloud.WordCloud(height = height, width = width)\n",
    "\twc.generate_from_frequencies(token_counts[[\"Bigram\", \"Difference\"]].sort_values(by = \"Difference\").iloc[0:WORDCLOUD_MAX_WORDS].values.tolist())\n",
    "\twc.to_file(vertical_name + \"_negative_wordcloud.png\")\n",
    "\n",
    "    \n",
    "## STEP 2: Getting it in a JSON format:\n",
    "def comment_extractor(df, vertical_name, ngram = 2, threshold = TF_WORD_THRESHOLD, ARTICLE_ALERT_SCORE_CUTOFF = ARTICLE_ALERT_SCORE_CUTOFF, comment_score_cutoff = COMMENT_SCORE_CUTOFF):\n",
    "\t'''\n",
    "\t\tThe important function. Comment Extraction is the mainstay of this package, isn't it?\n",
    "\t'''\n",
    "\ttokenized_message_list = tokenize(get_message_list(df), ngram = ngram)\n",
    "\ttoken_to_index_dict = get_token_to_index_dict(get_unnested_list(tokenized_message_list), threshold = threshold)\n",
    "\ttf_matrix = get_TF_matrix(tokenized_message_list, token_to_index_dict)\n",
    "\tidf_score = get_IDF_score(tf_matrix)\n",
    "\traw_comment_scores, LDA_positive_category = get_LDA_scores(tf_matrix, token_to_index_dict)\n",
    "\tfinal_comment_scores = compute_final_scores(raw_comment_scores, LDA_positive_category)\n",
    "\tfinal_post_scores = get_posts_scores_v2(new_df, raw_comment_scores, LDA_positive_category)\n",
    "\treturn get_articles(new_df, final_post_scores, raw_comment_scores, category, tf_matrix, idf_score, token_to_index_dict, comment_score_cutoff = comment_score_cutoff, ARTICLE_ALERT_SCORE_CUTOFF = ARTICLE_ALERT_SCORE_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## STEP 1: PREPROCESSING\n",
    "df = pd.read_csv(FILENAME, delimiter = ',')\n",
    "faulty_arrays = []\n",
    "good_arrays = []\n",
    "\n",
    "messageList = df[\"message\"].values.tolist()\n",
    "\n",
    "for i in range(len(messageList)):\n",
    "    message = messageList[i]\n",
    "    if not is_ASCII_string(ascii_substituted(str(message))):\n",
    "        faulty_arrays.append(i)\n",
    "    else:\n",
    "        good_arrays.append(i)\n",
    "        \n",
    "df = df.iloc[good_arrays, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_printout = []\n",
    "for category in df[\"page_vertical_tx\"].unique().tolist():\n",
    "\tnew_df = df[df[\"page_vertical_tx\"] == category]\n",
    "\tif new_df.shape[0] > 0:\n",
    "\t\textract = comment_extractor(new_df, category)\n",
    "\t\tprint(json.dumps(extract, indent = 4))\n",
    "\t\tjson_printout.append(extract)\n",
    "#generate_wordcloud(cc_df, \"Credit_cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertical:\n",
      "Credit Cards\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "There should be information about when we should receive the new Costo Citi card in our homes.  Give us the dates.  It is close to the expiration of my AMEX card, and I still haven't received my new Costo Visa card.  By what date do we need to contact Citi if we haven't received our card.\n",
      "Comment Text: \n",
      "Another question:  I'm a household member of the holder of my Costco membership; have my own membership card.  Can I get a Costco Anywhere Visa by Citi of my own or do I have to ask him to get such a card and be an authorized user on his card?\n",
      "Comment Text: \n",
      "I am trying to find an address for Citi bank Visa Card to give to my bank where I do online banking.  When I change from American Express to Visa I need an address for Citi bank.  I found a Sioux Falls, SD address on some paperwork, however, do not know if that is the one to use or not.\n",
      "post_csat_score\n",
      "0.194174757282\n",
      "post_url\n",
      "blog/credit-cards/costco-anywhere-visa-frequently-asked-questions\n",
      "post_id\n",
      "240846\n",
      "comment_count\n",
      "103\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['apply', 'how', 'new', 'get', 'want']\n",
      "Keywords of Topic 3\n",
      "['address', 'need', 'hand', 'not_received', 'citi']\n",
      "Keywords of Topic 2\n",
      "['not_want', 'what', 'use', 'card', 'go']\n",
      "post_alert_score\n",
      "0.811544290309\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "I do have a Costco Executive membership card & not expected to get a new Costco Citi card. How do I apply for a new Citi Visa card that will give me all the benefit as Costco/Citi card holders will get?\n",
      "Comment Text: \n",
      "I got an application from Citibank for the new card.  I was told that I don't need to apply for a card, but that it would be automatic.  What to do???\n",
      "Comment Text: \n",
      "I do not have a costco credit card. I just have a costco membership card  and a separate american express card. What do I do to get a visa card?\n",
      "post_csat_score\n",
      "0.222222222222\n",
      "post_url\n",
      "blog/credit-cards/costco-amex-cardholders-get-new-card-number-not-new-account\n",
      "post_id\n",
      "180640\n",
      "comment_count\n",
      "45\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['card', 'costco', 'get', 'what', '? ?']\n",
      "Keywords of Topic 2\n",
      "['call', 'activate', 'receive', 'member', 'want']\n",
      "post_alert_score\n",
      "0.794827484975\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "For my use this card is terrible.  Any card that claims to be an \"anywhere\" card and charges a rip-off 3% foreign exchange fee is being deceitful and dishonest.  Additionally, in this transfer I am going to experience a credit score drop because the limit on this card is $6,800 less than on my AMEX CostCo card which it replaces.  I will be voicing this loudly to CostCo and to Citi. There may even be a litigious angle to this situation.\n",
      "Comment Text: \n",
      "Read the Citibank Costco \"BUSINESS\" card terms of use.  Re: May be used only for business purchases and may never be used for personal purchases.  You didn't do your homework.  If city bank can show(easy to do) you used the card for any personal purchase they can change the interest rate or deny any and all rebates. Get a copy and read it.  Pretty\n",
      "Comment Text: \n",
      "I want to know HOW to apply for this card?  My AMEX was not a costco card so I need to apply for a citi visa but cannot find out how!\n",
      "post_csat_score\n",
      "0.433566433566\n",
      "post_url\n",
      "blog/credit-cards/new-citi-costco-visa-details\n",
      "post_id\n",
      "227319\n",
      "comment_count\n",
      "143\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['apply', 'how', 'card', 'visa card', 'citi']\n",
      "Keywords of Topic 2\n",
      "['card', 'use', 'confirm', 'may', 'active']\n",
      "post_alert_score\n",
      "0.780604729538\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Banking\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "I was looking for wair I. Could prepaid chase liquid card. Buying the card at what location can I Get the chase liquid card at.\n",
      "Comment Text: \n",
      "Need to know where one can be purchased without going to the bank to get one\n",
      "Comment Text: \n",
      "I don't live no where close to a chase bank. Please tell me how else can I get a chase liquid card\n",
      "post_csat_score\n",
      "0.409090909091\n",
      "post_url\n",
      "blog/banking/chase-liquid-prepaid-debit-card-review\n",
      "post_id\n",
      "214956\n",
      "comment_count\n",
      "44\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['chase', 'where', 'chase liquid', 'buy', 'get']\n",
      "Keywords of Topic 3\n",
      "['buy', 'know where', 'chase liquid', 'need', 'purchase']\n",
      "Keywords of Topic 2\n",
      "['card', 'chase liquid', 'purchase', 'buy', 'liquid card']\n",
      "post_alert_score\n",
      "0.859566252021\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "Didn't answer my question. If you received a prepaid debit card can you deposit it into your checking account?\n",
      "Comment Text: \n",
      "I want to know where to get one\n",
      "Comment Text: \n",
      "article lead me to believe there would be a comparison of options.....there was none.\n",
      "post_csat_score\n",
      "0.416666666667\n",
      "post_url\n",
      "blog/banking/prepaid-debit-cards-what-you-should-know\n",
      "post_id\n",
      "246321\n",
      "comment_count\n",
      "12\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['general', 'list', 'need', 'want', 'good work']\n",
      "Keywords of Topic 2\n",
      "['believe', 'receive', 'none', 'options', 'not_answer']\n",
      "post_alert_score\n",
      "0.82045573869\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "I wanted to learn how to get  & install software that would allow a business to cash a check without a trip to the bank. Also the cost of the software and the fee  per check.\n",
      "Comment Text: \n",
      "Cash a personal checks with out bank account and fee?\n",
      "Comment Text: \n",
      "What bank can I cash my check sent to me from alerus in othello wa, or tri cities wa,\n",
      "post_csat_score\n",
      "0\n",
      "post_url\n",
      "blog/banking/cash-check-paying-high-fees\n",
      "post_id\n",
      "175849\n",
      "comment_count\n",
      "12\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['cash', 'check', 'bank', 'cost', 'also']\n",
      "Keywords of Topic 2\n",
      "['not_answer', 'not_help', 'question', 'state', 'easy']\n",
      "post_alert_score\n",
      "0.801659882446\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Personal Finance\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "There is a 250.00 chrg in Maine for the initial visit to get a prescription.  200.00 if you are property stricken.  150.00 at the 6 month follow-up at the doctor.   150.00 upon the 1st year anniversary.  And 200.00 every year thereafter.  This is so dis heartening for those of use that could benefit and no longer wish to be Ginny pigs for the doctors who only know how to write prescriptions from the pharmaceutical companies that just want our money and are not invested in our well-being.\n",
      "Comment Text: \n",
      "I've had a card in Cali, but now live in Nevada. I need a card but financially unfortunately can't afford to at this time.  Wish there wasn't such a high fee, I am disabled and need cannabis.\n",
      "Comment Text: \n",
      "Gave everything I needed and wanted to know, quickly and in a well organized manner.\n",
      "post_csat_score\n",
      "0.384615384615\n",
      "post_url\n",
      "blog/finance/cost-medical-marijuana-card\n",
      "post_id\n",
      "137667\n",
      "comment_count\n",
      "13\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['maine', 'card', 'doctor', 'wish', 'need']\n",
      "Keywords of Topic 2\n",
      "['? ?', 'get', 'medical', 'nothing', 'how much']\n",
      "post_alert_score\n",
      "0.783002488991\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Small Business\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "My daughter wants, so very much, to open a small business. I wanted to help her by getting as much information as possible. Your site was the very first one i opened and I found ABSOLUTELY everything she could need to know. Thank you so much for making this information available. Even more, for presenting the info in simply language and formate for the older generation, such as myself. Thanks again.\n",
      "Comment Text: \n",
      "I just need a list of grants and the addresses .Most of these charge fees and if I had the money 'I would not be applying for grants.\n",
      "Comment Text: \n",
      "I have noticed all the requirements for obtaining a small business grant mean you have to already have an existing business and have been in business for 1-3 years.  Are there any options for start up costs for small businesses?\n",
      "post_csat_score\n",
      "0.384615384615\n",
      "post_url\n",
      "blog/small-business/small-business-grants-for-women\n",
      "post_id\n",
      "169316\n",
      "comment_count\n",
      "13\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['business', 'start', 'help', 'cost', 'work']\n",
      "post_alert_score\n",
      "0.71520911383\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Other\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Mortgages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Taxes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Loans\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "I work in financial aid and these articles do more harm than good. You need to tell students to stop applying to schools that are too much of a reach. That is why they aren't getting aid. Schools have need blind admissions, so they let in a bunch of students that are like a 6, 7, and 8 in their admissions rankings knowing that most can't afford it and will choose to go elsewhere while the rich students will still come. Those students that are middle of the road students should go to public colleges or less prestigious private schools. They will have a greater chance of getting more aid.\n",
      "Comment Text: \n",
      "Where is the section on WORKING YOUR WAY THROUGH COLLEGE?  I did it.  My spouse did it.  My children did it.  It's just NOT that hard to do, and you end up with a good college degree and don't have to take on a large amount of debt to repay.  Kids these days too lazy to work?  Wanting a free ride?  Hoping to party their way through school?  I think you need to rewrite this article to include a suggestion to WORK.  Sheesch!\n",
      "Comment Text: \n",
      "The entire topic of student aid is misleading. A loan is not aid: you can get a loan from the bank. And only 4% of aid comes from private scholarships, it's negligible at best\n",
      "post_csat_score\n",
      "0.129032258065\n",
      "post_url\n",
      "blog/loans/student-loans/not-enough-financial-aid\n",
      "post_id\n",
      "231829\n",
      "comment_count\n",
      "31\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['work', 'students', 'aid', 'go', 'school']\n",
      "Keywords of Topic 3\n",
      "['get', 'scholarships', 'useful', 'nothing', 'middle']\n",
      "Keywords of Topic 2\n",
      "['nothing new', 'simple', 'new', 'where', 'helpful']\n",
      "post_alert_score\n",
      "0.82808393806\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "All these lenders are for people with a credit score of 550 or better. What about those whose scores are lower than 550? How can anyone with a poor credit score get any help? Please find lenders who will give someone with a score less than 550 get the help they need. As far as I'm concerned credit scores are a joke.\n",
      "Comment Text: \n",
      "Lies about helping with bad credit. I have a job and make good money, I was laid off for a long whilegot behind, so credit is bad but I'm not. Nobody wants to help people, they only want to help the rich get richer.\n",
      "Comment Text: \n",
      "Nothing for people with a lower score, it always says they will help them you apply and they deny, stop making people believe that they can get the help\n",
      "post_csat_score\n",
      "0.211538461538\n",
      "post_url\n",
      "blog/loans/personal-loans-bad-credit\n",
      "post_id\n",
      "166771\n",
      "comment_count\n",
      "52\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['credit', 'credit score', 'people', 'score', 'lenders']\n",
      "Keywords of Topic 2\n",
      "['help', '550', 'credit', 'bad', 'score']\n",
      "post_alert_score\n",
      "0.798808445789\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Utilities\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "You fail to mention about grate customer service other than the grate prices that the other prepaid cellular company has. Some companies say that they are a better company, but lie to you and tell you something that you already know or don't want to hear. Half the time like Boost Mobile (who runs on the Sprint network), Strate Talk, TracFone Wireless (who are both affiliated companies) and other cellular companies don't have someone who cannot speak English very well. Half the time they will \"yes\" you to death or give you false information. If you ask for a supervisor sometime they cannot speak English as well (Boost Mobile is like that) and the Representatives will give you a \"song-and-a-dance\" and questions you on why you want a supervisor! In short, it's a catch 22 when you call in to these so-called \"cellular companies.\" So if your in doubt, ask the customer (the consumer) themselves.\n",
      "Comment Text: \n",
      "Didn't tell me anything more than what I already learned by going to the company's website. I wanted to know what plan is best...ie do calls get dropped a lot, do the phones work the same as a contract plan, etc...I actually wanted to know about the plans not just the bullet points taken from the company's website.\n",
      "Comment Text: \n",
      "I was teetering on which plan to take between Virgin and MetroPcs since I have used both and had great experience.  Your review has helped me decide.  Thank you!\n",
      "post_csat_score\n",
      "0.304347826087\n",
      "post_url\n",
      "blog/utilities/prepaid-cell-phone-plans\n",
      "post_id\n",
      "195008\n",
      "comment_count\n",
      "23\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['comparison', 'leave', 'show', 'list', 'deal']\n",
      "Keywords of Topic 2\n",
      "['company', 'well', 'half', 'who', 'ask']\n",
      "post_alert_score\n",
      "0.716174977205\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Insurance\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "Needs to update this to reflect modern day permanent life insurance policies that have additional features, such as overloan protection, chronic illness riders and much better IRR numbers over time than the traditional whole life policies.  He should have mentioned that most people that end up buying term insurance never invest the savings difference from purchasing whole life.  Not only does life insurance provide family protection, but can be designed to be a supplemental retirement plan that has the potential to provide a better retirement.  Poor article and misleading....in fact the author should know better.\n",
      "Comment Text: \n",
      "It is not a complete explanation... Where is the mention of continued annual growth to pay that interest due??? Where is the mention of dividend withdrawals (potentially tax free to cost base)? Everyone has a bias and this article demonstrates authors...\n",
      "Comment Text: \n",
      "Explain the investment side. Most average Americans who are purchasing whole life have never heard of this and has not been informed about such or how it works,therefore, is meaningless.\n",
      "post_csat_score\n",
      "0.166666666667\n",
      "post_url\n",
      "blog/insurance/tax-consequences-whole-life-insurance\n",
      "post_id\n",
      "261664\n",
      "comment_count\n",
      "12\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['mention', 'author', 'bias', 'term', 'whole life']\n",
      "post_alert_score\n",
      "0.878923903477\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "You can't use insurance rates.  In MA, insurance rates are fixed by the state.  There is no way to compare a rate in Springfield to a comparable policy in Hartford, CT (about 30 minutes away).\n",
      "Comment Text: \n",
      "If you're going to make a list list them all\n",
      "Comment Text: \n",
      "Need to be able to put in city you are interested in , say LA, and get ranking\n",
      "post_csat_score\n",
      "0.384615384615\n",
      "post_url\n",
      "blog/insurance/dangerous-cities-car-drivers-2016\n",
      "post_id\n",
      "245791\n",
      "comment_count\n",
      "13\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['city', 'put', 'interest', '?', 'say']\n",
      "post_alert_score\n",
      "0.831804937135\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Shopping\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "doesn't say when it ends which is what I was looking for\n",
      "Comment Text: \n",
      "Next time give a date for when the sale is over\n",
      "Comment Text: \n",
      "when is the end date of the sale would be helpful\n",
      "post_csat_score\n",
      "0.318181818182\n",
      "post_url\n",
      "blog/shopping/victorias-secret-semi-annual-sale-guide\n",
      "post_id\n",
      "154816\n",
      "comment_count\n",
      "22\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['when', 'end', 'sale', 'not_tell', 'look']\n",
      "post_alert_score\n",
      "0.957598791374\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "If you don't already know .. Victoria's Secret is going to discontinue the Victoria's Secret line of swimwear this year .. probably by the end of 2016. So .. with this next semi-annual sale .. don't be surprised if they offer even deeper discounts on the VS line!\n",
      "Comment Text: \n",
      "I need to know if the sale is over yet///June 20th\n",
      "Comment Text: \n",
      "Does not tell me when the sale ends.\n",
      "post_csat_score\n",
      "0.379310344828\n",
      "post_url\n",
      "blog/shopping/victorias-secret-semi-annual-sale-date-2016\n",
      "post_id\n",
      "260411\n",
      "comment_count\n",
      "29\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['end', 'sale', 'victoria', 'not_tell', 'when']\n",
      "post_alert_score\n",
      "0.81930143677\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vertical:\n",
      "Investing\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "While the article made some good points and offered many good topics for people to consider - I think there are too many errors, oversights and shortcomings in the material to be truly \"helpful\".  First of all - I totally disagree on the order of #1 & #2 - never pass up known (free) money to fuel the unknown \"maybe\" of emergency funds. Good topics - wrong order.  Secondly, the assumption that a 20 or 30-something truly knows how much they will need at retirement is far-fetched for the masses. Instead, a means for determining that and a tool for helping them to find a balance of investment vehicles may be more useful.  And finally - not one mention of the most important thing... minimizing debt - living \"beneath\" your means to break the cycle of the growing hoards that may never be able to afford to retire because they have fallen victim to the ME and NOW generational tendencies that cannot even fathom how to begin to do what your article is suggesting.\n",
      "Comment Text: \n",
      "401K should be the first thing to savings because it is tax avoidance.  You put in money that is before taxes that makes you believe you have a larger balance and you feel better.   Money is withdraw at a pace which you don''t even feel it. You can borrow from your 401K, so it could be act like an emergency fund.   Think twice before what, everyone needs to put money in their 401K, the more they can the better that is the facts..    This article should not be even written why do you hire idiots to writ these foolish things.  At the end of the article the facts are not about why we need an emergency fund yet it is about how much should one invest in 401K where did the subject change.\n",
      "Comment Text: \n",
      "contributing to retirement is essential in this current economic climate. also, contributing early is key. waiting to start contributing to retirement until after you have an established 6 month emergency fund is negligent advice. it may take years to save up for the 6 month fund, and that would be years of lost contributions to retirement. also, someone in their 20s saving for this kids college fund is a poor example. this article seems like a rushed story or a fluff piece used to fill up space. the lazy information provided could be harmful to people seeking to learn more about retirement contributions.\n",
      "post_csat_score\n",
      "0.03125\n",
      "post_url\n",
      "blog/investing/think-twice-maxing-401k\n",
      "post_id\n",
      "242403\n",
      "comment_count\n",
      "96\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['401k', 'fund', 'max', 'k', '401']\n",
      "Keywords of Topic 2\n",
      "['k', 'ira', 'roth', 'roth ira', 'efund']\n",
      "post_alert_score\n",
      "0.774144775133\n",
      "\n",
      "\n",
      "comment\n",
      "Comment Text: \n",
      "Pseudo science.   At the end of the day, Dave Rowan can do these overly complex calculations and be no closer to the answer.  There are many assumptions implicitly built into his analysis that will introduce error.  The assumption of an annual 3% raise is an example.  Unless you are a government employee it is impossible to forecast your final salary many years out.  The entire assumption of basing your retirement needs on your salary is deeply flawed.  In order to create substantial net worth you need to live on a fraction of your income.  This fraction will vary widely as you try to project your necessary net worth and how much you need to tighten your belt to achieve that future net worth.  The 4% rule is still the best guidance available.  Everyone's primary focus should be on increasing their net worth.  It is very simple for investors to look at their net worth each year, multiply by 4% and ask themselves if they are on track to a comfortable retirement.\n",
      "Comment Text: \n",
      "Why do Financial Planners always ignore that fact that peoples' spending rates will decrease in retirement? If people are not saving for retirement, their spending rates will remain at the current level in retirement. Everybody that I talk to about retirement that are actively saving will have at least their mortgage and vehicles paid off before retiring. Thus eliminating a major chunk of expenses that will not be replaced by other consumables.\n",
      "Comment Text: \n",
      "Too much science.  Just invest 10% of what you make, 20% if you make over 150K, payoff your mortgage early, then immediately start saving an extra 15%  of your net pay, Oh by the way, keep your car 2 or 3 years once you paid it off, and invest the car payment too. Once your kids are on their own, you will be living on about 60 to 65 percent of your net pay.  You should be more than OK.\n",
      "post_csat_score\n",
      "0.131782945736\n",
      "post_url\n",
      "blog/investing/tell-youre-track-retirement\n",
      "post_id\n",
      "257227\n",
      "comment_count\n",
      "129\n",
      "keywords\n",
      "Keywords of Topic 1\n",
      "['complicate', 'retirement', '200,000', 'math', 'salary']\n",
      "Keywords of Topic 3\n",
      "['need', 'net worth', 'pension', 'add', '4']\n",
      "Keywords of Topic 2\n",
      "['confuse', 'understand', 'formulas', 'calculations', 'figure']\n",
      "post_alert_score\n",
      "0.76869408943\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in json_printout:\n",
    "    for key in item.keys():\n",
    "        if key != \"summary\":\n",
    "            print key + \":\"\n",
    "        value = item[key]\n",
    "        if type(value) is str:\n",
    "            print value\n",
    "        if len(value) > 0 and key == \"summary\":\n",
    "            for subkey in value:\n",
    "                if type(subkey) is dict:\n",
    "                    for attributes in subkey.keys():\n",
    "                        print attributes\n",
    "                        if type(subkey[attributes]) is dict:\n",
    "                            for keywords in subkey[attributes].keys():\n",
    "                                print keywords\n",
    "                                print subkey[attributes][keywords]\n",
    "                        elif type(subkey[attributes]) is list:\n",
    "                            for i in range(len(subkey[attributes])):\n",
    "                                print \"Comment Text: \"\n",
    "                                print subkey[attributes][i][\"comment_text\"]\n",
    "                        else:\n",
    "                            print subkey[attributes]\n",
    "                print \"\\n\"\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
